{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval 2022 Task 2 : Subtask A [Baseline]",
      "provenance": [],
      "collapsed_sections": [
        "do-TXGBemGgH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mekjr1/google-colabs/blob/main/SemEval_2022_Task_2_Subtask_A_%5BBaseline%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9b_p85gxaGc"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook provides a baseline for each setting in [Subtask A of SemEval 2022 Task 2](https://sites.google.com/view/semeval2022task2-idiomaticity#h.qq7eefmehqf9). In addition this provides some helpful pre-processing scripts that you are free to use with your experiments. \n",
        "\n",
        "Please start by stepping through this notebook so you have a clear idea as to what is expected of the task and what you need to submit. \n",
        "\n",
        "These baselines are based on the results described in the paper “[AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models](https://arxiv.org/abs/2109.04413)”. \n",
        "\n",
        "## Zero-shot setting: Methodology \n",
        "\n",
        "Note that in the zero-shot setting you are NOT allowed to train the model using the one-shot data. \n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). This is based on the results presented in the dataset paper. \n",
        "\n",
        "We use Multilingual BERT for this setting.\n",
        "\n",
        "## One-shot setting: Methodology\n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. Again, this is based on the results presented in the dataset paper. \n",
        "\n",
        "We also use Multilingual BERT for this setting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-TXGBemGgH"
      },
      "source": [
        "# Setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsITUAnzvFl"
      },
      "source": [
        "Download the Task data and evaluation scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3qhQdpl-1-",
        "outputId": "7606d743-6823-40a3-eb64-f9fcaff2cd4c"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval_2022_Task2-idiomaticity'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 72 (delta 21), reused 60 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0POB9tzfNx"
      },
      "source": [
        "Download the “AStitchInLanguageModels” code which we make use of. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affNQCRktdx4",
        "outputId": "60fc820d-51c7-481c-f355-e5f0918b6a05"
      },
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AStitchInLanguageModels'...\n",
            "remote: Enumerating objects: 1012, done.\u001b[K\n",
            "remote: Counting objects: 100% (1012/1012), done.\u001b[K\n",
            "remote: Compressing objects: 100% (754/754), done.\u001b[K\n",
            "remote: Total 1012 (delta 370), reused 804 (delta 202), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1012/1012), 79.86 MiB | 27.63 MiB/s, done.\n",
            "Resolving deltas: 100% (370/370), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60w-An2vzikk"
      },
      "source": [
        "Download and install an editable version of huggingfaces transformers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8BhcLYcmVvd",
        "outputId": "cb0935d9-a95d-415b-de4b-d80bd68924dd"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 83486, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 83486 (delta 16), reused 31 (delta 9), pack-reused 83438\u001b[K\n",
            "Receiving objects: 100% (83486/83486), 66.67 MiB | 25.20 MiB/s, done.\n",
            "Resolving deltas: 100% (59911/59911), done.\n",
            "/content/transformers\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 77.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.11.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.11.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.11.0.dev0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVMnwTSzmjJ"
      },
      "source": [
        "Required for run_glue ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsWits5tw1t",
        "outputId": "71c7e873-1817-4cfe-af58-eae8489fb57d"
      },
      "source": [
        "## run_glue needs this. \n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20 kB 44.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30 kB 44.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 40 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 61 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 71 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 112 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 122 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 133 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 143 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 163 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 174 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 184 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 194 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 204 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 215 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 225 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 235 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 245 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 264 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.16)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.8.1 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igYdTTgzp9e"
      },
      "source": [
        "Editable install requires runtime restart unless we do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOuKplBmmbeB"
      },
      "source": [
        "import site\n",
        "site.main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvC8kAGNnKk_"
      },
      "source": [
        "# Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOw3MaG7nN77"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzDtW9eXnOhG"
      },
      "source": [
        "def load_csv( path, delimiter=',' ) : \n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter ) \n",
        "    for row in reader : \n",
        "      if header is None : \n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row ) \n",
        "  return header, data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtDsdtAnSZu"
      },
      "source": [
        "def write_csv( data, location ) : \n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile ) \n",
        "    writer.writerows( data ) \n",
        "  print( \"Wrote {}\".format( location ) ) \n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Io3D3_z4wt"
      },
      "source": [
        "The following function creates a submission file from the predictions output by run_glue (the text classification script from huggingface transformers - see below). \n",
        "\n",
        "Note that we set it up so we can load up results for only one setting. \n",
        "\n",
        "It requires as input the submission format file, which is available with the data. You can call this after completing each setting to load up results for both settings (see below).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re31vnLoQWww"
      },
      "source": [
        "def insert_to_submission_file( submission_format_file, input_file, prediction_format_file, setting ) :\n",
        "    submission_header, submission_content = load_csv( submission_format_file )\n",
        "    input_header     , input_data         = load_csv( input_file             )\n",
        "    prediction_header, prediction_data    = load_csv( prediction_format_file, '\\t' )\n",
        "\n",
        "    assert len( input_data ) == len( prediction_data )\n",
        "\n",
        "    ## submission_header ['ID', 'Language', 'Setting', 'Label']\n",
        "    ## input_header      ['label', 'sentence1' ]\n",
        "    ## prediction_header ['index', 'prediction']\n",
        "\n",
        "    prediction_data = list( reversed( prediction_data ) )\n",
        "\n",
        "    started_insert  = False\n",
        "    for elem in submission_content : \n",
        "        if elem[ submission_header.index( 'Setting' ) ] != setting :\n",
        "            if started_insert :\n",
        "                if len( prediction_data ) == 0 :\n",
        "                    break\n",
        "                else : \n",
        "                    raise Exception( \"Update should to contiguous ... something wrong.\" ) \n",
        "            continue\n",
        "        started_insert = True\n",
        "        elem[ submission_header.index( 'Label' ) ] = prediction_data.pop()[ prediction_header.index( 'prediction' ) ]\n",
        "\n",
        "    return [ submission_header ] + submission_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LyZ-OXmgQW"
      },
      "source": [
        "# Pre-process: Create train and dev and evaluation data in required format\n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”). \n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3ymBcEmxaV"
      },
      "source": [
        "## Functions for pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthVK7EQm6m_"
      },
      "source": [
        "### _get_train_data\n",
        "\n",
        "This function generates training data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPGq-Y1Jmvv5"
      },
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom ) :\n",
        "    \n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "        \n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ header.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytociCB3WZM"
      },
      "source": [
        "### _get_dev_eval_data\n",
        "\n",
        "This function generates training dev and eval data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters. \n",
        "\n",
        "Additionally, if there is no gold label provides (as in the case of eval) it will generate a file that can be used to generate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe4YQJ9Sm-B2"
      },
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None : \n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "    \n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None : \n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "            \n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "            \n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        assert len( out_header ) == len( this_row ) \n",
        "        out_data.append( this_row )\n",
        "        \n",
        "\n",
        "    return [ out_header ] + out_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIbyTnn3fHP"
      },
      "source": [
        "### create_data\n",
        "\n",
        "This function generates the training, development and evaluation data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1tr-zNvnBCV"
      },
      "source": [
        "\"\"\"\n",
        "Based on the results presented in `AStitchInLanguageModels' we work with not including the idiom for the zero shot setting and including it in the one shot setting.\n",
        "\"\"\"\n",
        "def create_data( input_location, output_location ) :\n",
        "\n",
        "    \n",
        "    ## Zero shot data\n",
        "    train_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False\n",
        "    )\n",
        "    write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    ## OneShot Data (combine both for training)\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmQfvym8ndKH"
      },
      "source": [
        "## Setup and Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCgaHlKnpMR",
        "outputId": "6d216c97-dad6-4ec7-9e6b-cf9484dcd220"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AStitchInLanguageModels  SemEval_2022_Task2-idiomaticity\n",
            "sample_data\t\t transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeKLg-Hngs4",
        "outputId": "7ec6726f-295c-4e43-b5a4-127560c3ffe0"
      },
      "source": [
        "outpath = 'Data'\n",
        "    \n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/train.csv\n",
            "Wrote Data/ZeroShot/dev.csv\n",
            "Wrote Data/ZeroShot/eval.csv\n",
            "Wrote Data/OneShot/train.csv\n",
            "Wrote Data/OneShot/dev.csv\n",
            "Wrote Data/OneShot/eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-Ol7hfoC8a"
      },
      "source": [
        "# Zero Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-GiQvnkoL67"
      },
      "source": [
        "## Train Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0BwA0uoKAu",
        "outputId": "8f439b38-1815-4adf-c070-053b34fc9e5f"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 18:53:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 18:53:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Sep03_18-53-40_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=0,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 18:53:40 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "09/03/2021 18:53:40 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "09/03/2021 18:53:41 - WARNING - datasets.builder -   Using custom data configuration default-114e87a6b4086e67\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-114e87a6b4086e67/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-114e87a6b4086e67/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1665] 2021-09-03 18:53:41,731 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppvwr5i3v\n",
            "Downloading: 100% 625/625 [00:00<00:00, 667kB/s]\n",
            "[INFO|file_utils.py:1669] 2021-09-03 18:53:42,090 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|file_utils.py:1677] 2021-09-03 18:53:42,090 >> creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 18:53:42,091 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 18:53:42,091 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2021-09-03 18:53:42,444 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdud8ie93\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 30.9kB/s]\n",
            "[INFO|file_utils.py:1669] 2021-09-03 18:53:42,799 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1677] 2021-09-03 18:53:42,799 >> creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 18:53:43,153 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 18:53:43,153 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2021-09-03 18:53:43,867 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpukg_v00p\n",
            "Downloading: 100% 996k/996k [00:00<00:00, 2.34MB/s]\n",
            "[INFO|file_utils.py:1669] 2021-09-03 18:53:44,739 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1677] 2021-09-03 18:53:44,739 >> creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|file_utils.py:1665] 2021-09-03 18:53:45,093 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgpw_au_x\n",
            "Downloading: 100% 1.96M/1.96M [00:00<00:00, 3.76MB/s]\n",
            "[INFO|file_utils.py:1669] 2021-09-03 18:53:46,070 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|file_utils.py:1677] 2021-09-03 18:53:46,071 >> creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 18:53:47,135 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 18:53:47,135 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 18:53:47,135 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 18:53:47,135 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 18:53:47,135 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 18:53:47,497 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 18:53:47,498 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1665] 2021-09-03 18:53:47,999 >> https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1wrxkzy8\n",
            "Downloading: 100% 714M/714M [00:17<00:00, 40.8MB/s]\n",
            "[INFO|file_utils.py:1669] 2021-09-03 18:54:05,682 >> storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|file_utils.py:1677] 2021-09-03 18:54:05,683 >> creating metadata file for /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[INFO|modeling_utils.py:1279] 2021-09-03 18:54:05,683 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:1516] 2021-09-03 18:54:07,720 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1527] 2021-09-03 18:54:07,720 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 5/5 [00:00<00:00,  5.95ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.29ba/s]\n",
            "09/03/2021 18:54:08 - INFO - __main__ -   Sample 3155 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 14156, 10114, 11486, 119, 10212, 117, 10105, 20169, 10410, 11940, 10134, 15556, 21756, 10155, 34624, 50575, 10901, 117, 10473, 10531, 30419, 12172, 10472, 24278, 10105, 16672, 11356, 11444, 18703, 30159, 100, 187, 38587, 10106, 10827, 19376, 119, 30159, 11059, 13745, 10105, 20169, 10410, 10924, 61637, 10114, 14045, 10686, 10105, 18077, 117, 10319, 10134, 10873, 40851, 10106, 88651, 10169, 10751, 22975, 10978, 10105, 39189, 100, 187, 17090, 10155, 23874, 22392, 10708, 10105, 47723, 11630, 61637, 10189, 11951, 78275, 18745, 119, 21194, 119, 10386, 105315, 14234, 11598, 10855, 117, 12373, 10105, 39189, 29914, 10454, 39575, 25385, 119, 10258, 13990, 10114, 17876, 10474, 17090, 12166, 10105, 42230, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'According to history.com, the leap day was originally discovered by Egyptian astronomers, but this discovery did not reach the western world until Julius Caesar’s reign in 45 BC. Caesar then created the leap year calendar to fix the problem, which was later adapted in accordance with new knowledge about the earth’s orbit by Pope Gregory into the Gregorian calendar that we observe today. Feb. 29 happens every four years, because the earth technically requires 365.25 days to complete its orbit around the sun.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/03/2021 18:54:08 - INFO - __main__ -   Sample 3445 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 138, 70747, 10143, 34862, 117, 12943, 18965, 41583, 117, 59216, 10104, 77302, 10121, 24457, 14732, 10147, 21367, 10220, 10427, 10614, 64440, 173, 10321, 22849, 10220, 12656, 12114, 13360, 100, 22798, 10212, 47097, 19075, 10220, 10427, 66130, 10107, 87537, 10143, 72154, 27187, 10242, 95322, 49323, 113, 46743, 114, 117, 169, 11419, 118, 107041, 10149, 13218, 27062, 18965, 41583, 82389, 10138, 25574, 25819, 10104, 15395, 64440, 173, 99702, 10121, 10303, 35474, 10147, 11793, 28223, 10266, 11675, 10104, 57833, 169, 54728, 10143, 17857, 119, 152, 20229, 117, 12943, 12593, 117, 10448, 183, 26219, 10567, 55227, 16686, 10242, 10139, 104915, 110285, 55167, 102453, 10107, 11793, 36818, 23821, 10104, 10106, 63996, 11498, 10266, 23571, 55167, 117, 10402, 34338, 169, 54728, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'A saída da crise, segundo Bachelet, depende de ações que garantam renda para os mais pobres e vacina para todos São Paulo – Alta comissária para os Direitos Humanos da Organização das Nações Unidas (ONU), a ex-presidenta do Chile Michelle Bachelet criticou governantes de países pobres e ricos que optaram pela economia em vez de promover a saúde da população. O resultado, segundo ela, foi o aprofundamento das desigualdades sociais causadas pela histórica falta de investimento em áreas sociais, entre elas a saúde.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/03/2021 18:54:08 - INFO - __main__ -   Sample 331 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 77961, 80677, 41131, 117, 10106, 36944, 10169, 144, 11011, 10731, 117, 10529, 14628, 169, 10751, 13170, 10108, 53730, 52828, 10111, 99024, 11327, 26483, 10841, 76456, 81635, 74062, 92233, 112, 187, 119, 10576, 10725, 34062, 10160, 10105, 25000, 39039, 10858, 20924, 25539, 12357, 10195, 117, 77961, 80677, 41131, 10309, 46948, 10336, 10169, 10105, 107, 33671, 20924, 93218, 10108, 10105, 13567, 107, 17725, 117, 10142, 10105, 20206, 80677, 41131, 119, 10117, 34713, 10134, 23736, 10160, 10105, 22712, 53329, 20640, 30328, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'Concept Smoke Screen, in partnership with G4S, have developed a new way of defending cash and guards against attacks when replenishing ATM\\'s. On May 11th at the IFSEC Security Industry Awards 2009, Concept Smoke Screen were honoured with the \"Physical Security Product of the Year\" award, for the Guardian Smoke Screen. The ceremony was conducted at the Birmingham Hilton Metropole.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:521] 2021-09-03 18:54:15,168 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:1168] 2021-09-03 18:54:15,178 >> ***** Running training *****\n",
            "[INFO|trainer.py:1169] 2021-09-03 18:54:15,178 >>   Num examples = 4491\n",
            "[INFO|trainer.py:1170] 2021-09-03 18:54:15,178 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1171] 2021-09-03 18:54:15,178 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1172] 2021-09-03 18:54:15,178 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1173] 2021-09-03 18:54:15,178 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1174] 2021-09-03 18:54:15,178 >>   Total optimization steps = 1269\n",
            " 11% 141/1269 [00:55<06:10,  3.04it/s][INFO|trainer.py:521] 2021-09-03 18:55:11,077 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 18:55:11,079 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 18:55:11,079 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 18:55:11,079 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.18it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.43it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.92it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.43it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.10it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.77it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.72it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.69it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.65it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.62it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.63it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.66it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.52it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.53it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.53it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.56it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.60it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.50it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.56it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.58it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.61it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.62it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.64it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.55it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.53it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.40it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.46it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7149094343185425, 'eval_accuracy': 0.6143437027931213, 'eval_f1': 0.6099989445167646, 'eval_runtime': 3.3744, 'eval_samples_per_second': 219.001, 'eval_steps_per_second': 27.56, 'epoch': 1.0}\n",
            " 11% 141/1269 [00:59<06:10,  3.04it/s]\n",
            "100% 93/93 [00:03<00:00, 27.51it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 18:55:14,455 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-141\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 18:55:14,456 >> Configuration saved in models/ZeroShot/0/checkpoint-141/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 18:55:17,043 >> Model weights saved in models/ZeroShot/0/checkpoint-141/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 18:55:17,044 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-141/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 18:55:17,044 >> Special tokens file saved in models/ZeroShot/0/checkpoint-141/special_tokens_map.json\n",
            " 22% 282/1269 [02:04<05:23,  3.05it/s][INFO|trainer.py:521] 2021-09-03 18:56:19,707 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 18:56:19,709 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 18:56:19,709 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 18:56:19,709 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.89it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.87it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.32it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.70it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.37it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.00it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.72it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.70it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.67it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.67it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.68it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.69it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.68it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.66it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.61it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.49it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.51it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.39it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.47it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.52it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.59it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.60it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.64it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.57it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.59it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.60it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.60it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.0462379455566406, 'eval_accuracy': 0.6129904985427856, 'eval_f1': 0.612393632629108, 'eval_runtime': 3.3572, 'eval_samples_per_second': 220.121, 'eval_steps_per_second': 27.701, 'epoch': 2.0}\n",
            " 22% 282/1269 [02:07<05:23,  3.05it/s]\n",
            "100% 93/93 [00:03<00:00, 27.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 18:56:23,068 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-282\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 18:56:23,069 >> Configuration saved in models/ZeroShot/0/checkpoint-282/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 18:56:25,548 >> Model weights saved in models/ZeroShot/0/checkpoint-282/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 18:56:25,549 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-282/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 18:56:25,549 >> Special tokens file saved in models/ZeroShot/0/checkpoint-282/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 18:56:32,437 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-141] due to args.save_total_limit\n",
            " 33% 423/1269 [03:13<04:37,  3.04it/s][INFO|trainer.py:521] 2021-09-03 18:57:28,434 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 18:57:28,437 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 18:57:28,437 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 18:57:28,437 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.32it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.62it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.23it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.71it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.37it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.05it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.94it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.86it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.68it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.68it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.69it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.68it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.58it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.59it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.63it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.63it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.53it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.39it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.43it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.49it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.50it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.57it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.58it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.61it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.60it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.58it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.61it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.171615719795227, 'eval_accuracy': 0.6589986681938171, 'eval_f1': 0.6566592920353982, 'eval_runtime': 3.3595, 'eval_samples_per_second': 219.975, 'eval_steps_per_second': 27.683, 'epoch': 3.0}\n",
            " 33% 423/1269 [03:16<04:37,  3.04it/s]\n",
            "100% 93/93 [00:03<00:00, 27.59it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 18:57:31,839 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-423\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 18:57:31,840 >> Configuration saved in models/ZeroShot/0/checkpoint-423/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 18:57:34,368 >> Model weights saved in models/ZeroShot/0/checkpoint-423/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 18:57:34,368 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-423/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 18:57:34,368 >> Special tokens file saved in models/ZeroShot/0/checkpoint-423/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 18:57:41,418 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-282] due to args.save_total_limit\n",
            "{'loss': 0.3062, 'learning_rate': 1.2119779353821908e-05, 'epoch': 3.55}\n",
            " 44% 564/1269 [04:22<03:50,  3.06it/s][INFO|trainer.py:521] 2021-09-03 18:58:37,202 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 18:58:37,204 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 18:58:37,204 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 18:58:37,204 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.24it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.42it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.11it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.60it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.31it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.10it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.95it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.84it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.79it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.75it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.60it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.63it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.64it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.61it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.64it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.55it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.56it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.60it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.50it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.59it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.58it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.61it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.61it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.64it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.64it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.53it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.458234429359436, 'eval_accuracy': 0.6495263576507568, 'eval_f1': 0.6495238199872921, 'eval_runtime': 3.355, 'eval_samples_per_second': 220.269, 'eval_steps_per_second': 27.72, 'epoch': 4.0}\n",
            " 44% 564/1269 [04:25<03:50,  3.06it/s]\n",
            "100% 93/93 [00:03<00:00, 27.57it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 18:58:40,561 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-564\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 18:58:40,561 >> Configuration saved in models/ZeroShot/0/checkpoint-564/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 18:58:43,044 >> Model weights saved in models/ZeroShot/0/checkpoint-564/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 18:58:43,045 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-564/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 18:58:43,046 >> Special tokens file saved in models/ZeroShot/0/checkpoint-564/special_tokens_map.json\n",
            " 56% 705/1269 [05:31<03:04,  3.06it/s][INFO|trainer.py:521] 2021-09-03 18:59:46,236 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 18:59:46,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 18:59:46,239 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 18:59:46,239 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.35it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.71it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.27it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.71it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.39it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.17it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 28.00it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.88it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.69it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.67it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.67it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.68it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.66it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.65it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.67it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.67it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.59it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.55it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.58it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.62it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.64it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.47it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.9162631034851074, 'eval_accuracy': 0.6792963743209839, 'eval_f1': 0.6655157069712636, 'eval_runtime': 3.3549, 'eval_samples_per_second': 220.275, 'eval_steps_per_second': 27.721, 'epoch': 5.0}\n",
            " 56% 705/1269 [05:34<03:04,  3.06it/s]\n",
            "100% 93/93 [00:03<00:00, 27.45it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 18:59:49,595 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-705\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 18:59:49,596 >> Configuration saved in models/ZeroShot/0/checkpoint-705/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 18:59:52,177 >> Model weights saved in models/ZeroShot/0/checkpoint-705/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 18:59:52,178 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-705/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 18:59:52,178 >> Special tokens file saved in models/ZeroShot/0/checkpoint-705/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 18:59:59,135 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-423] due to args.save_total_limit\n",
            "[INFO|trainer.py:2011] 2021-09-03 18:59:59,169 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-564] due to args.save_total_limit\n",
            " 67% 846/1269 [06:39<02:17,  3.07it/s][INFO|trainer.py:521] 2021-09-03 19:00:54,713 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:00:54,715 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:00:54,715 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:00:54,715 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.63it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.79it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.30it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.76it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.40it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.18it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 28.05it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.89it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.79it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.77it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.72it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.68it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.67it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.69it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.64it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.65it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.66it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.65it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.66it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.67it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.66it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.65it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.66it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.67it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.48it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.52it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.55it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.59it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.0821917057037354, 'eval_accuracy': 0.6738836169242859, 'eval_f1': 0.6730190553300397, 'eval_runtime': 3.3497, 'eval_samples_per_second': 220.618, 'eval_steps_per_second': 27.764, 'epoch': 6.0}\n",
            " 67% 846/1269 [06:42<02:17,  3.07it/s]\n",
            "100% 93/93 [00:03<00:00, 27.57it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:00:58,066 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-846\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:00:58,067 >> Configuration saved in models/ZeroShot/0/checkpoint-846/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:01:00,581 >> Model weights saved in models/ZeroShot/0/checkpoint-846/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:01:00,581 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-846/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:01:00,582 >> Special tokens file saved in models/ZeroShot/0/checkpoint-846/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:01:07,568 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-705] due to args.save_total_limit\n",
            " 78% 987/1269 [07:47<01:31,  3.08it/s][INFO|trainer.py:521] 2021-09-03 19:02:03,062 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:02:03,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:02:03,064 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:02:03,064 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.90it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.84it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.34it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.78it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.42it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.12it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.97it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.86it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.79it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.77it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.75it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.73it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.72it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.68it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.59it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.61it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.51it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.56it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.59it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.63it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.67it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.69it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.69it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.70it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.64it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.53it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.56it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.60it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.2197368144989014, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6678480378431748, 'eval_runtime': 3.3493, 'eval_samples_per_second': 220.642, 'eval_steps_per_second': 27.767, 'epoch': 7.0}\n",
            " 78% 987/1269 [07:51<01:31,  3.08it/s]\n",
            "100% 93/93 [00:03<00:00, 27.53it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:02:06,415 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-987\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:02:06,416 >> Configuration saved in models/ZeroShot/0/checkpoint-987/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:02:08,940 >> Model weights saved in models/ZeroShot/0/checkpoint-987/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:02:08,940 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:02:08,941 >> Special tokens file saved in models/ZeroShot/0/checkpoint-987/special_tokens_map.json\n",
            "{'loss': 0.0281, 'learning_rate': 4.239558707643815e-06, 'epoch': 7.09}\n",
            " 89% 1128/1269 [08:56<00:45,  3.07it/s][INFO|trainer.py:521] 2021-09-03 19:03:11,272 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:03:11,274 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:03:11,274 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:03:11,274 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.34it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.67it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.97it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.52it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.26it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.05it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.93it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.84it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.68it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.66it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.50it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.43it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.50it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.56it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.58it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.59it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.60it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.60it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.56it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.46it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.50it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.56it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.58it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.60it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.62it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.66it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.65it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.319486141204834, 'eval_accuracy': 0.665764570236206, 'eval_f1': 0.6642274672979889, 'eval_runtime': 3.3582, 'eval_samples_per_second': 220.06, 'eval_steps_per_second': 27.694, 'epoch': 8.0}\n",
            " 89% 1128/1269 [08:59<00:45,  3.07it/s]\n",
            "100% 93/93 [00:03<00:00, 27.64it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:03:14,634 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1128\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:03:14,634 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:03:17,190 >> Model weights saved in models/ZeroShot/0/checkpoint-1128/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:03:17,191 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1128/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:03:17,191 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1128/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:03:24,183 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-987] due to args.save_total_limit\n",
            "100% 1269/1269 [10:04<00:00,  3.08it/s][INFO|trainer.py:521] 2021-09-03 19:04:19,698 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:04:19,700 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:04:19,700 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:04:19,700 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.83it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.86it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.19it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.64it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.33it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 28.11it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.96it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.85it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.78it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.75it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.72it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.50it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.56it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.54it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.59it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.62it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.65it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.67it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.68it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.56it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.60it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.61it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.60it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.54it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.59it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.63it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.61it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.64it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.3422296047210693, 'eval_accuracy': 0.6630581617355347, 'eval_f1': 0.6619663198966854, 'eval_runtime': 3.3512, 'eval_samples_per_second': 220.517, 'eval_steps_per_second': 27.751, 'epoch': 9.0}\n",
            "100% 1269/1269 [10:07<00:00,  3.08it/s]\n",
            "100% 93/93 [00:03<00:00, 27.67it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:04:23,054 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1269\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:04:23,054 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:04:25,574 >> Model weights saved in models/ZeroShot/0/checkpoint-1269/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:04:25,575 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1269/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:04:25,575 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1269/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:04:32,691 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1128] due to args.save_total_limit\n",
            "[INFO|trainer.py:1366] 2021-09-03 19:04:32,835 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1375] 2021-09-03 19:04:32,835 >> Loading best model from models/ZeroShot/0/checkpoint-846 (score: 0.6730190553300397).\n",
            "{'train_runtime': 630.1692, 'train_samples_per_second': 64.14, 'train_steps_per_second': 2.014, 'train_loss': 0.1330403221038686, 'epoch': 9.0}\n",
            "100% 1269/1269 [10:30<00:00,  2.01it/s]\n",
            "[INFO|trainer.py:1935] 2021-09-03 19:04:45,521 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:04:45,522 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:04:47,922 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:04:47,922 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:04:47,923 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =      0.133\n",
            "  train_runtime            = 0:10:30.16\n",
            "  train_samples            =       4491\n",
            "  train_samples_per_second =      64.14\n",
            "  train_steps_per_second   =      2.014\n",
            "09/03/2021 19:04:48 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:04:48,156 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:04:48,158 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:04:48,158 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:04:48,158 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.73it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.6739\n",
            "  eval_f1                 =      0.673\n",
            "  eval_loss               =     2.0822\n",
            "  eval_runtime            = 0:00:03.38\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    218.548\n",
            "  eval_steps_per_second   =     27.503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrfwDiORPDyS",
        "outputId": "fd468b6a-810e-4743-8e39-794a5bb9de2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb2Uo0vPPc3"
      },
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/ZeroShot/0/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etL-Ic6bPmtA"
      },
      "source": [
        "## Bring back saved model here. \n",
        "#!mkdir -p /content/models/ZeroShot/0/\n",
        "# !cp -r /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/* /content/models/ZeroShot/0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bN4iUHWP45b"
      },
      "source": [
        "## Evaluation On Dev Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "houOZpcYO-Pw",
        "outputId": "e8583e14-6ffc-4c18-c718-98400b394f30"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/ZeroShot/0' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/eval-dev/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "      --test_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 19:07:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 19:07:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/eval-dev/runs/Sep03_19-07-08_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/ZeroShot/0/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=eval-dev,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 19:07:08 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "09/03/2021 19:07:08 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "09/03/2021 19:07:08 - INFO - __main__ -   load a local file for test: Data/ZeroShot/dev.csv\n",
            "09/03/2021 19:07:09 - WARNING - datasets.builder -   Using custom data configuration default-451ae46b7c76267e\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-451ae46b7c76267e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-451ae46b7c76267e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:559] 2021-09-03 19:07:09,268 >> loading configuration file /content/models/ZeroShot/0/config.json\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:07:09,269 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1671] 2021-09-03 19:07:09,270 >> Didn't find file /content/models/ZeroShot/0/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:07:09,271 >> loading file /content/models/ZeroShot/0/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:07:09,271 >> loading file /content/models/ZeroShot/0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:07:09,271 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:07:09,271 >> loading file /content/models/ZeroShot/0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:07:09,271 >> loading file /content/models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1277] 2021-09-03 19:07:09,424 >> loading weights file /content/models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1524] 2021-09-03 19:07:11,820 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1533] 2021-09-03 19:07:11,820 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/ZeroShot/0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "100% 5/5 [00:01<00:00,  4.64ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.39ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.09ba/s]\n",
            "09/03/2021 19:07:19 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:07:19,934 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:07:19,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:07:19,937 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:07:19,937 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.89it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6739\n",
            "  eval_f1                 =      0.673\n",
            "  eval_loss               =     2.0822\n",
            "  eval_runtime            = 0:00:03.45\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    214.116\n",
            "  eval_steps_per_second   =     26.946\n",
            "09/03/2021 19:07:23 - INFO - __main__ -   *** Test ***\n",
            "/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py:514: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  test_dataset.remove_columns_(\"label\")\n",
            "[INFO|trainer.py:521] 2021-09-03 19:07:23,392 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:07:23,394 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:07:23,394 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:07:23,394 >>   Batch size = 8\n",
            " 97% 90/93 [00:03<00:00, 27.72it/s]09/03/2021 19:07:26 - INFO - __main__ -   ***** Test results None *****\n",
            "100% 93/93 [00:03<00:00, 27.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHqPYuTS3muJ"
      },
      "source": [
        "### Use predictions to create the submission file (for dev data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWUuwg7Qm-t"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_submission_format.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/ZeroShot/0/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'zero_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgFGlGnTROJZ"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXcRbv70RZfR"
      },
      "source": [
        "!mkdir -p outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ezIzyWTRePp",
        "outputId": "7de89b60-8070-46de-b295-fa90cf847955"
      },
      "source": [
        "write_csv( updated_data, 'outputs/zero_shot_dev_formated.csv' ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/zero_shot_dev_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpFkOSQ3x_A"
      },
      "source": [
        "### For the development data, we can run evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "Dn6MyP9jRnFA",
        "outputId": "a4e5e3e6-287b-4de6-c643-e569e9c2b42e"
      },
      "source": [
        "import sys\n",
        "sys.path.append( '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/' ) \n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/zero_shot_dev_formated.csv'\n",
        "gold_file       = '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n0.6617513092193399],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n0.6385562974902141],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n0.6730190553300397],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n[null, null, null]],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n[null, null, null]],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n[null, null, null]]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"string\", \"F1 Score (Macro)\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.661751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.638556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.673019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Settings Languages    F1 Score (Macro)\n",
              "0  zero_shot        EN            0.661751\n",
              "1  zero_shot        PT            0.638556\n",
              "2  zero_shot     EN,PT            0.673019\n",
              "3   one_shot        EN  (None, None, None)\n",
              "4   one_shot        PT  (None, None, None)\n",
              "5   one_shot     EN,PT  (None, None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UX9ys7tTKVi"
      },
      "source": [
        "## Generate Eval Data output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ets4flitTRZZ",
        "outputId": "1deda94d-73b9-4955-c0d4-4999e3b306ee"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/ZeroShot/0' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/eval-eval/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "      --test_file Data/ZeroShot/eval.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 19:13:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 19:13:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/eval-eval/runs/Sep03_19-13-53_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/ZeroShot/0/eval-eval/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=eval-eval,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/eval-eval/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 19:13:53 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train.csv\n",
            "09/03/2021 19:13:53 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "09/03/2021 19:13:53 - INFO - __main__ -   load a local file for test: Data/ZeroShot/eval.csv\n",
            "09/03/2021 19:13:53 - WARNING - datasets.builder -   Using custom data configuration default-c7581e3ebff4090e\n",
            "09/03/2021 19:13:53 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-c7581e3ebff4090e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "[INFO|configuration_utils.py:559] 2021-09-03 19:13:53,583 >> loading configuration file /content/models/ZeroShot/0/config.json\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:13:53,584 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1671] 2021-09-03 19:13:53,584 >> Didn't find file /content/models/ZeroShot/0/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:13:53,585 >> loading file /content/models/ZeroShot/0/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:13:53,585 >> loading file /content/models/ZeroShot/0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:13:53,585 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:13:53,585 >> loading file /content/models/ZeroShot/0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:13:53,585 >> loading file /content/models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1277] 2021-09-03 19:13:53,678 >> loading weights file /content/models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1524] 2021-09-03 19:13:55,564 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1533] 2021-09-03 19:13:55,564 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/ZeroShot/0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "100% 5/5 [00:01<00:00,  4.88ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.32ba/s]\n",
            "100% 1/1 [00:00<00:00,  6.88ba/s]\n",
            "09/03/2021 19:13:59 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:13:59,991 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:13:59,993 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:13:59,993 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:13:59,993 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.86it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6739\n",
            "  eval_f1                 =      0.673\n",
            "  eval_loss               =     2.0822\n",
            "  eval_runtime            = 0:00:03.38\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    218.476\n",
            "  eval_steps_per_second   =     27.494\n",
            "09/03/2021 19:14:03 - INFO - __main__ -   *** Test ***\n",
            "/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py:514: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  test_dataset.remove_columns_(\"label\")\n",
            "[INFO|trainer.py:521] 2021-09-03 19:14:03,380 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:14:03,382 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:14:03,382 >>   Num examples = 762\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:14:03,382 >>   Batch size = 8\n",
            " 97% 93/96 [00:03<00:00, 27.70it/s]09/03/2021 19:14:06 - INFO - __main__ -   ***** Test results None *****\n",
            "100% 96/96 [00:03<00:00, 27.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSS6PFfb4AAl"
      },
      "source": [
        "### Use predictions to create the submission file (for eval data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqxzrRBfTcnq"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval_submission_format.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/ZeroShot/0/eval-eval/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'zero_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHqanuVDTz-r"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKDBuxMLT5QU",
        "outputId": "f5eefcfb-ace6-4225-f64a-92bba8dd3ec0"
      },
      "source": [
        "write_csv( updated_data, 'outputs/zero_shot_eval_formated.csv' ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/zero_shot_eval_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyWXLvHI4Cdt"
      },
      "source": [
        "**NOTE**: You can submit this file, but it only has results for the zero-shot setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7Irn_YvIni"
      },
      "source": [
        "# One Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVYvUH3OvR9b"
      },
      "source": [
        "## Train One shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQO751yzvVJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355f4d09-4507-4040-8874-20526e0fe4e7"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 19:14:57 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 19:14:57 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Sep03_19-14-57_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=1,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 19:14:57 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "09/03/2021 19:14:57 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "09/03/2021 19:14:58 - WARNING - datasets.builder -   Using custom data configuration default-b20fc29361f294b2\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-b20fc29361f294b2/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b20fc29361f294b2/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 19:14:58,763 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:14:58,764 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 19:14:59,471 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:14:59,472 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 19:15:01,612 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 19:15:01,612 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 19:15:01,612 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 19:15:01,612 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-09-03 19:15:01,612 >> loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|configuration_utils.py:561] 2021-09-03 19:15:01,966 >> loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:15:01,967 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1279] 2021-09-03 19:15:02,457 >> loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
            "[WARNING|modeling_utils.py:1516] 2021-09-03 19:15:13,950 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1527] 2021-09-03 19:15:13,951 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 5/5 [00:00<00:00,  9.97ba/s]\n",
            "100% 1/1 [00:00<00:00, 12.28ba/s]\n",
            "09/03/2021 19:15:14 - INFO - __main__ -   Sample 1100 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 90491, 14796, 11299, 19573, 10108, 54883, 63658, 10992, 72812, 14197, 11850, 10114, 52843, 10741, 117, 11257, 90739, 10105, 13448, 11192, 10142, 11345, 10114, 47413, 10455, 42399, 10124, 24523, 119, 102, 54883, 63658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'Given how many hours of beauty sleep most kitties like to clock up, choosing the right place for them to lay their heads is critical.', 'sentence2': 'beauty sleep', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/03/2021 19:15:14 - INFO - __main__ -   Sample 516 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 71547, 54118, 11997, 45982, 117, 10479, 10491, 20262, 10107, 11922, 94460, 87748, 10142, 11056, 27577, 18051, 117, 21937, 10105, 10106, 56914, 117, 100, 66434, 11155, 10590, 169, 19118, 78681, 117, 10374, 16796, 10111, 43816, 65884, 10135, 10114, 10105, 19118, 78681, 117, 15127, 32282, 10124, 10189, 12916, 156, 13078, 10537, 10894, 10529, 39803, 55911, 17781, 11345, 119, 100, 102, 19118, 78681, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'Yet Sergeant Mark Brady, who oversees major collision investigations for South Yorkshire Police, told the inquiry, “Had there been a hard shoulder, had Jason and Alexandru pulled on to the hard shoulder, my opinion is that Mr Szuba would have driven clean past them.”', 'sentence2': 'hard shoulder', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/03/2021 19:15:14 - INFO - __main__ -   Sample 2089 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 14600, 10105, 11365, 41807, 65896, 28278, 12014, 10188, 48448, 10708, 169, 100766, 12221, 10142, 13847, 30665, 18511, 113, 30776, 10891, 114, 10114, 25470, 11008, 12166, 10531, 10924, 117, 68339, 11830, 10124, 45749, 10474, 187, 15983, 10157, 21158, 13893, 10169, 169, 11366, 10108, 23604, 10135, 83019, 25955, 119, 102, 100766, 12221, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'sentence1': 'While the family fun hub holds back from turning into a ghost town for Japanese Yokai (demons) to roam around this year, Downtown East is bringing its scary stories online with a series of episodes on Instagram Stories.', 'sentence2': 'ghost town', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:521] 2021-09-03 19:15:17,315 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:1168] 2021-09-03 19:15:17,322 >> ***** Running training *****\n",
            "[INFO|trainer.py:1169] 2021-09-03 19:15:17,322 >>   Num examples = 4631\n",
            "[INFO|trainer.py:1170] 2021-09-03 19:15:17,322 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1171] 2021-09-03 19:15:17,322 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1172] 2021-09-03 19:15:17,322 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1173] 2021-09-03 19:15:17,323 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1174] 2021-09-03 19:15:17,323 >>   Total optimization steps = 1305\n",
            " 11% 145/1305 [00:57<07:09,  2.70it/s][INFO|trainer.py:521] 2021-09-03 19:16:14,992 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:16:14,994 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:16:14,994 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:16:14,995 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.54it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.63it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.07it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.54it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.16it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.83it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.61it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.43it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.42it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.41it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.10it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.14it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.21it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.30it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.36it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.40it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.44it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.46it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.46it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.49it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.49it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.43it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.47it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.47it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.30it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.31it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7504113912582397, 'eval_accuracy': 0.7604871392250061, 'eval_f1': 0.7604713551919573, 'eval_runtime': 3.3857, 'eval_samples_per_second': 218.273, 'eval_steps_per_second': 27.469, 'epoch': 1.0}\n",
            " 11% 145/1305 [01:01<07:09,  2.70it/s]\n",
            "100% 93/93 [00:03<00:00, 27.21it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:16:18,381 >> Saving model checkpoint to models/OneShot/1/checkpoint-145\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:16:18,382 >> Configuration saved in models/OneShot/1/checkpoint-145/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:16:20,929 >> Model weights saved in models/OneShot/1/checkpoint-145/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:16:20,929 >> tokenizer config file saved in models/OneShot/1/checkpoint-145/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:16:20,930 >> Special tokens file saved in models/OneShot/1/checkpoint-145/special_tokens_map.json\n",
            " 22% 290/1305 [02:07<06:16,  2.69it/s][INFO|trainer.py:521] 2021-09-03 19:17:25,052 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:17:25,056 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:17:25,056 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:17:25,056 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.64it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.46it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.01it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.48it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.17it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.95it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.82it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.73it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.64it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.60it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.57it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.51it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.40it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.45it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.46it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.45it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.42it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.44it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.45it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.33it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.29it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.36it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.35it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.40it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8677113056182861, 'eval_accuracy': 0.8010825514793396, 'eval_f1': 0.8007541895855075, 'eval_runtime': 3.3752, 'eval_samples_per_second': 218.953, 'eval_steps_per_second': 27.554, 'epoch': 2.0}\n",
            " 22% 290/1305 [02:11<06:16,  2.69it/s]\n",
            "100% 93/93 [00:03<00:00, 27.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:17:28,432 >> Saving model checkpoint to models/OneShot/1/checkpoint-290\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:17:28,433 >> Configuration saved in models/OneShot/1/checkpoint-290/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:17:30,758 >> Model weights saved in models/OneShot/1/checkpoint-290/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:17:30,759 >> tokenizer config file saved in models/OneShot/1/checkpoint-290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:17:30,759 >> Special tokens file saved in models/OneShot/1/checkpoint-290/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:17:37,439 >> Deleting older checkpoint [models/OneShot/1/checkpoint-145] due to args.save_total_limit\n",
            " 33% 435/1305 [03:17<05:21,  2.71it/s][INFO|trainer.py:521] 2021-09-03 19:18:35,027 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:18:35,030 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:18:35,030 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:18:35,030 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.44it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.36it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.98it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.48it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.15it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.92it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.76it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.68it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.62it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.45it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.43it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.39it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.40it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.24it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.28it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.34it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.35it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.37it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.41it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.44it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.47it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.48it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.45it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.36it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8037542104721069, 'eval_accuracy': 0.824086606502533, 'eval_f1': 0.8203197474339408, 'eval_runtime': 3.3784, 'eval_samples_per_second': 218.745, 'eval_steps_per_second': 27.528, 'epoch': 3.0}\n",
            " 33% 435/1305 [03:21<05:21,  2.71it/s]\n",
            "100% 93/93 [00:03<00:00, 27.40it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:18:38,412 >> Saving model checkpoint to models/OneShot/1/checkpoint-435\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:18:38,413 >> Configuration saved in models/OneShot/1/checkpoint-435/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:18:40,768 >> Model weights saved in models/OneShot/1/checkpoint-435/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:18:40,768 >> tokenizer config file saved in models/OneShot/1/checkpoint-435/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:18:40,769 >> Special tokens file saved in models/OneShot/1/checkpoint-435/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:18:47,489 >> Deleting older checkpoint [models/OneShot/1/checkpoint-290] due to args.save_total_limit\n",
            "{'loss': 0.1476, 'learning_rate': 1.2337164750957855e-05, 'epoch': 3.45}\n",
            " 44% 580/1305 [04:27<04:27,  2.71it/s][INFO|trainer.py:521] 2021-09-03 19:19:45,125 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:19:45,127 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:19:45,127 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:19:45,127 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.60it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.41it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.02it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.51it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.19it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.96it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.81it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.71it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.62it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.58it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.44it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.46it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.47it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.43it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.20it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.29it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.35it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.33it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.36it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.40it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.45it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.43it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.42it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.23it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.7923168540000916, 'eval_accuracy': 0.8592692613601685, 'eval_f1': 0.857642439060532, 'eval_runtime': 3.3783, 'eval_samples_per_second': 218.747, 'eval_steps_per_second': 27.528, 'epoch': 4.0}\n",
            " 44% 580/1305 [04:31<04:27,  2.71it/s]\n",
            "100% 93/93 [00:03<00:00, 27.31it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:19:48,507 >> Saving model checkpoint to models/OneShot/1/checkpoint-580\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:19:48,508 >> Configuration saved in models/OneShot/1/checkpoint-580/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:19:51,013 >> Model weights saved in models/OneShot/1/checkpoint-580/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:19:51,013 >> tokenizer config file saved in models/OneShot/1/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:19:51,013 >> Special tokens file saved in models/OneShot/1/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:19:57,805 >> Deleting older checkpoint [models/OneShot/1/checkpoint-435] due to args.save_total_limit\n",
            " 56% 725/1305 [05:37<03:33,  2.72it/s][INFO|trainer.py:521] 2021-09-03 19:20:55,310 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:20:55,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:20:55,313 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:20:55,313 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.71it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.68it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.14it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.58it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.07it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.88it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.63it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.59it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.55it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.52it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.51it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.52it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.51it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.48it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.48it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.37it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.27it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.34it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.30it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.31it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.38it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.44it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.46it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.47it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.47it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.48it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.8172897100448608, 'eval_accuracy': 0.8687415719032288, 'eval_f1': 0.8676861738945854, 'eval_runtime': 3.3764, 'eval_samples_per_second': 218.871, 'eval_steps_per_second': 27.544, 'epoch': 5.0}\n",
            " 56% 725/1305 [05:41<03:33,  2.72it/s]\n",
            "100% 93/93 [00:03<00:00, 27.34it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:20:58,691 >> Saving model checkpoint to models/OneShot/1/checkpoint-725\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:20:58,692 >> Configuration saved in models/OneShot/1/checkpoint-725/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:21:01,154 >> Model weights saved in models/OneShot/1/checkpoint-725/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:21:01,155 >> tokenizer config file saved in models/OneShot/1/checkpoint-725/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:21:01,155 >> Special tokens file saved in models/OneShot/1/checkpoint-725/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:21:07,887 >> Deleting older checkpoint [models/OneShot/1/checkpoint-580] due to args.save_total_limit\n",
            " 67% 870/1305 [06:47<02:40,  2.71it/s][INFO|trainer.py:521] 2021-09-03 19:22:05,301 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:22:05,303 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:22:05,303 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:22:05,303 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.63it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.55it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.98it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.32it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.03it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.74it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.64it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.60it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.56it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.52it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.51it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.49it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.33it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.37it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.39it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.42it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.36it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.40it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.43it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.46it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.43it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.29it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.30it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.34it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.39it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.39it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.30it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.822790265083313, 'eval_accuracy': 0.874154269695282, 'eval_f1': 0.8738206313099324, 'eval_runtime': 3.3819, 'eval_samples_per_second': 218.519, 'eval_steps_per_second': 27.5, 'epoch': 6.0}\n",
            " 67% 870/1305 [06:51<02:40,  2.71it/s]\n",
            "100% 93/93 [00:03<00:00, 27.30it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:22:08,686 >> Saving model checkpoint to models/OneShot/1/checkpoint-870\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:22:08,687 >> Configuration saved in models/OneShot/1/checkpoint-870/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:22:11,221 >> Model weights saved in models/OneShot/1/checkpoint-870/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:22:11,221 >> tokenizer config file saved in models/OneShot/1/checkpoint-870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:22:11,221 >> Special tokens file saved in models/OneShot/1/checkpoint-870/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:22:18,029 >> Deleting older checkpoint [models/OneShot/1/checkpoint-725] due to args.save_total_limit\n",
            "{'loss': 0.0163, 'learning_rate': 4.674329501915709e-06, 'epoch': 6.9}\n",
            " 78% 1015/1305 [07:58<01:46,  2.72it/s][INFO|trainer.py:521] 2021-09-03 19:23:15,399 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:23:15,401 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:23:15,401 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:23:15,401 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.31it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.33it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.86it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.20it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 27.83it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.64it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.54it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.23it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.18it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.12it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.19it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.24it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.17it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.26it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.32it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.37it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.41it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.45it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.47it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.44it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.40it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.41it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.34it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.38it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.41it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.35it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9129729270935059, 'eval_accuracy': 0.8687415719032288, 'eval_f1': 0.8672627986600918, 'eval_runtime': 3.3926, 'eval_samples_per_second': 217.826, 'eval_steps_per_second': 27.412, 'epoch': 7.0}\n",
            " 78% 1015/1305 [08:01<01:46,  2.72it/s]\n",
            "100% 93/93 [00:03<00:00, 27.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:23:18,795 >> Saving model checkpoint to models/OneShot/1/checkpoint-1015\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:23:18,796 >> Configuration saved in models/OneShot/1/checkpoint-1015/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:23:21,224 >> Model weights saved in models/OneShot/1/checkpoint-1015/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:23:21,225 >> tokenizer config file saved in models/OneShot/1/checkpoint-1015/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:23:21,225 >> Special tokens file saved in models/OneShot/1/checkpoint-1015/special_tokens_map.json\n",
            " 89% 1160/1305 [09:07<00:53,  2.72it/s][INFO|trainer.py:521] 2021-09-03 19:24:25,111 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:24:25,114 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:24:25,114 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:24:25,114 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.62it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.57it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 28.92it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.44it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.12it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.74it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.60it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.58it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.56it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.46it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.38it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.39it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.24it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.33it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.37it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.34it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.38it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.42it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.41it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.41it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.42it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.35it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.36it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.33it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.35it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.38it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.32it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.38it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9974929094314575, 'eval_accuracy': 0.8660351634025574, 'eval_f1': 0.8642816728965894, 'eval_runtime': 3.3837, 'eval_samples_per_second': 218.403, 'eval_steps_per_second': 27.485, 'epoch': 8.0}\n",
            " 89% 1160/1305 [09:11<00:53,  2.72it/s]\n",
            "100% 93/93 [00:03<00:00, 27.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:24:28,499 >> Saving model checkpoint to models/OneShot/1/checkpoint-1160\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:24:28,500 >> Configuration saved in models/OneShot/1/checkpoint-1160/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:24:30,993 >> Model weights saved in models/OneShot/1/checkpoint-1160/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:24:30,994 >> tokenizer config file saved in models/OneShot/1/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:24:30,994 >> Special tokens file saved in models/OneShot/1/checkpoint-1160/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:24:37,778 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1015] due to args.save_total_limit\n",
            "100% 1305/1305 [10:17<00:00,  2.72it/s][INFO|trainer.py:521] 2021-09-03 19:25:35,163 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:25:35,165 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:25:35,166 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:25:35,166 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 4/93 [00:00<00:02, 36.59it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:02, 30.59it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:02, 29.09it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:02, 28.49it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 28.12it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 27.86it/s]\u001b[A\n",
            " 26% 24/93 [00:00<00:02, 27.59it/s]\u001b[A\n",
            " 29% 27/93 [00:00<00:02, 27.57it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:02, 27.52it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:02, 27.50it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 27.40it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:01, 27.42it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 27.44it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 27.45it/s]\u001b[A\n",
            " 52% 48/93 [00:01<00:01, 27.45it/s]\u001b[A\n",
            " 55% 51/93 [00:01<00:01, 27.47it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 27.36it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 27.31it/s]\u001b[A\n",
            " 65% 60/93 [00:02<00:01, 27.29it/s]\u001b[A\n",
            " 68% 63/93 [00:02<00:01, 27.30it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 27.35it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 27.28it/s]\u001b[A\n",
            " 77% 72/93 [00:02<00:00, 27.26it/s]\u001b[A\n",
            " 81% 75/93 [00:02<00:00, 27.30it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 27.36it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 27.40it/s]\u001b[A\n",
            " 90% 84/93 [00:03<00:00, 27.44it/s]\u001b[A\n",
            " 94% 87/93 [00:03<00:00, 27.44it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.9357128739356995, 'eval_accuracy': 0.8700947165489197, 'eval_f1': 0.868884453315591, 'eval_runtime': 3.3806, 'eval_samples_per_second': 218.597, 'eval_steps_per_second': 27.51, 'epoch': 9.0}\n",
            "100% 1305/1305 [10:21<00:00,  2.72it/s]\n",
            "100% 93/93 [00:03<00:00, 27.46it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1935] 2021-09-03 19:25:38,550 >> Saving model checkpoint to models/OneShot/1/checkpoint-1305\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:25:38,551 >> Configuration saved in models/OneShot/1/checkpoint-1305/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:25:41,009 >> Model weights saved in models/OneShot/1/checkpoint-1305/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:25:41,010 >> tokenizer config file saved in models/OneShot/1/checkpoint-1305/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:25:41,010 >> Special tokens file saved in models/OneShot/1/checkpoint-1305/special_tokens_map.json\n",
            "[INFO|trainer.py:2011] 2021-09-03 19:25:47,755 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1160] due to args.save_total_limit\n",
            "[INFO|trainer.py:1366] 2021-09-03 19:25:47,893 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1375] 2021-09-03 19:25:47,894 >> Loading best model from models/OneShot/1/checkpoint-870 (score: 0.8738206313099324).\n",
            "{'train_runtime': 642.5989, 'train_samples_per_second': 64.86, 'train_steps_per_second': 2.031, 'train_loss': 0.06321473254097833, 'epoch': 9.0}\n",
            "100% 1305/1305 [10:42<00:00,  2.03it/s]\n",
            "[INFO|trainer.py:1935] 2021-09-03 19:25:59,975 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:391] 2021-09-03 19:25:59,976 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-03 19:26:02,625 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-03 19:26:02,625 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-03 19:26:02,626 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.0632\n",
            "  train_runtime            = 0:10:42.59\n",
            "  train_samples            =       4631\n",
            "  train_samples_per_second =      64.86\n",
            "  train_steps_per_second   =      2.031\n",
            "09/03/2021 19:26:02 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:26:02,842 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:26:02,844 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:26:02,844 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:26:02,844 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.65it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.8742\n",
            "  eval_f1                 =     0.8738\n",
            "  eval_loss               =     0.8228\n",
            "  eval_runtime            = 0:00:03.40\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    217.346\n",
            "  eval_steps_per_second   =     27.352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW8stSsnIKWo",
        "outputId": "a797d11f-5b2f-4854-883f-fa1d592dafe1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0uO16BfcIur"
      },
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/OneShot/1/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dijAXZ7dD5V"
      },
      "source": [
        "## Evaluation On Dev Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2903I4hKdJuE",
        "outputId": "76a28edb-007c-457e-b3a9-8530d8ef5d68"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/OneShot/1' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/eval-dev/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "      --test_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 19:28:44 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 19:28:44 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/eval-dev/runs/Sep03_19-28-43_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/OneShot/1/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=eval-dev,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 19:28:44 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "09/03/2021 19:28:44 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "09/03/2021 19:28:44 - INFO - __main__ -   load a local file for test: Data/OneShot/dev.csv\n",
            "09/03/2021 19:28:44 - WARNING - datasets.builder -   Using custom data configuration default-295a48b2f725460e\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-295a48b2f725460e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-295a48b2f725460e/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:559] 2021-09-03 19:28:44,616 >> loading configuration file /content/models/OneShot/1/config.json\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:28:44,617 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1671] 2021-09-03 19:28:44,618 >> Didn't find file /content/models/OneShot/1/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:28:44,618 >> loading file /content/models/OneShot/1/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:28:44,618 >> loading file /content/models/OneShot/1/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:28:44,618 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:28:44,618 >> loading file /content/models/OneShot/1/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:28:44,618 >> loading file /content/models/OneShot/1/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1277] 2021-09-03 19:28:44,801 >> loading weights file /content/models/OneShot/1/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1524] 2021-09-03 19:28:56,235 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1533] 2021-09-03 19:28:56,235 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/OneShot/1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "100% 5/5 [00:00<00:00,  8.47ba/s]\n",
            "100% 1/1 [00:00<00:00, 12.81ba/s]\n",
            "100% 1/1 [00:00<00:00, 12.90ba/s]\n",
            "09/03/2021 19:29:03 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:29:03,224 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:29:03,227 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:29:03,227 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:29:03,227 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.83it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.8742\n",
            "  eval_f1                 =     0.8738\n",
            "  eval_loss               =     0.8228\n",
            "  eval_runtime            = 0:00:03.45\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    213.752\n",
            "  eval_steps_per_second   =       26.9\n",
            "09/03/2021 19:29:06 - INFO - __main__ -   *** Test ***\n",
            "/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py:514: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  test_dataset.remove_columns_(\"label\")\n",
            "[INFO|trainer.py:521] 2021-09-03 19:29:06,688 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:29:06,690 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:29:06,690 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:29:06,690 >>   Batch size = 8\n",
            " 97% 90/93 [00:03<00:00, 27.62it/s]09/03/2021 19:29:10 - INFO - __main__ -   ***** Test results None *****\n",
            "100% 93/93 [00:03<00:00, 27.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjobV-x4R_8"
      },
      "source": [
        "### Use predictions to create the submission file (for dev data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZIEpIstdg_j"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/outputs/zero_shot_dev_formated.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/OneShot/1/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'one_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrS0hvEDdspS",
        "outputId": "9d87a4b1-6d24-47ce-8277-b426ec562b43"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )\n",
        " write_csv( updated_data, 'outputs/both_dev_formated.csv' ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/both_dev_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB2cBSmA4ZSR"
      },
      "source": [
        "### For the development data, we can run evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "vTWEOw3Qd0Tz",
        "outputId": "4fff9308-e5e0-4a30-f0d2-ef4634363cb0"
      },
      "source": [
        "import sys\n",
        "sys.path.append( '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/' ) \n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/both_dev_formated.csv'\n",
        "gold_file       = '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n{\n            'v': 0.6617513092193399,\n            'f': \"0.6617513092193399\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n{\n            'v': 0.6385562974902141,\n            'f': \"0.6385562974902141\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n{\n            'v': 0.6730190553300397,\n            'f': \"0.6730190553300397\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n{\n            'v': 0.8704885668832538,\n            'f': \"0.8704885668832538\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n{\n            'v': 0.8665745046290478,\n            'f': \"0.8665745046290478\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n{\n            'v': 0.8738206313099324,\n            'f': \"0.8738206313099324\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"number\", \"F1 Score (Macro)\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.661751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.638556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.673019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.870489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.866575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.873821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Settings Languages  F1 Score (Macro)\n",
              "0  zero_shot        EN          0.661751\n",
              "1  zero_shot        PT          0.638556\n",
              "2  zero_shot     EN,PT          0.673019\n",
              "3   one_shot        EN          0.870489\n",
              "4   one_shot        PT          0.866575\n",
              "5   one_shot     EN,PT          0.873821"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhQptb1ePms"
      },
      "source": [
        "## Generate Eval Data output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81G4ULAseSPB",
        "outputId": "9c1dcaec-c058-4f3b-c963-9cf040e854e1"
      },
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/OneShot/1' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/eval-eval/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "      --test_file Data/OneShot/eval.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 19:30:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2021 19:30:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/eval-eval/runs/Sep03_19-30-26_eb29b3e472c9,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "output_dir=models/OneShot/1/eval-eval/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=eval-eval,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/eval-eval/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "09/03/2021 19:30:26 - INFO - __main__ -   load a local file for train: Data/OneShot/train.csv\n",
            "09/03/2021 19:30:26 - INFO - __main__ -   load a local file for validation: Data/OneShot/dev.csv\n",
            "09/03/2021 19:30:26 - INFO - __main__ -   load a local file for test: Data/OneShot/eval.csv\n",
            "09/03/2021 19:30:26 - WARNING - datasets.builder -   Using custom data configuration default-ad0327cc5e3be10a\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-ad0327cc5e3be10a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ad0327cc5e3be10a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:559] 2021-09-03 19:30:26,476 >> loading configuration file /content/models/OneShot/1/config.json\n",
            "[INFO|configuration_utils.py:598] 2021-09-03 19:30:26,477 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1671] 2021-09-03 19:30:26,477 >> Didn't find file /content/models/OneShot/1/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:30:26,477 >> loading file /content/models/OneShot/1/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:30:26,477 >> loading file /content/models/OneShot/1/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:30:26,477 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:30:26,478 >> loading file /content/models/OneShot/1/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1739] 2021-09-03 19:30:26,478 >> loading file /content/models/OneShot/1/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1277] 2021-09-03 19:30:26,573 >> loading weights file /content/models/OneShot/1/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1524] 2021-09-03 19:30:28,482 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:1533] 2021-09-03 19:30:28,482 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/OneShot/1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "100% 5/5 [00:00<00:00,  8.50ba/s]\n",
            "100% 1/1 [00:00<00:00, 13.17ba/s]\n",
            "100% 1/1 [00:00<00:00, 12.88ba/s]\n",
            "09/03/2021 19:30:32 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:521] 2021-09-03 19:30:32,105 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:30:32,108 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:30:32,108 >>   Num examples = 739\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:30:32,108 >>   Batch size = 8\n",
            "100% 93/93 [00:03<00:00, 27.77it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.8742\n",
            "  eval_f1                 =     0.8738\n",
            "  eval_loss               =     0.8228\n",
            "  eval_runtime            = 0:00:03.39\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    217.831\n",
            "  eval_steps_per_second   =     27.413\n",
            "09/03/2021 19:30:35 - INFO - __main__ -   *** Test ***\n",
            "/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py:514: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  test_dataset.remove_columns_(\"label\")\n",
            "[INFO|trainer.py:521] 2021-09-03 19:30:35,504 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1.\n",
            "[INFO|trainer.py:2181] 2021-09-03 19:30:35,507 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2183] 2021-09-03 19:30:35,507 >>   Num examples = 762\n",
            "[INFO|trainer.py:2186] 2021-09-03 19:30:35,507 >>   Batch size = 8\n",
            " 97% 93/96 [00:03<00:00, 27.74it/s]09/03/2021 19:30:38 - INFO - __main__ -   ***** Test results None *****\n",
            "100% 96/96 [00:03<00:00, 27.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnOf3yts4gcb"
      },
      "source": [
        "### Use predictions to create the submission file (for eval data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaG_XF6JJNuV"
      },
      "source": [
        "#### Create One Shot submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk1EaW7IJa3A"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval_submission_format.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/OneShot/1/eval-eval/test_results_None.txt'                         ,\n",
        "    }\n",
        "params[ 'setting' ] = 'one_shot'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IjSoShpJv6z",
        "outputId": "71d6fc47-8164-46b4-ccc2-44e171cbe30e"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )\n",
        " write_csv( updated_data, 'outputs/one_shot_eval_formated.csv' ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/one_shot_eval_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZlTBYSiJRP6"
      },
      "source": [
        "#### Combine Zero Shot and One Shot submission files.\n",
        "\n",
        "Do this by loading zero shot data as submission file format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwpGn7d3edI1"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/outputs/zero_shot_eval_formated.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/eval.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/OneShot/1/eval-eval/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'one_shot'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "440OwBxyfJJs",
        "outputId": "99274dd4-42b0-41c5-d390-b511f5ee4c9c"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )\n",
        " write_csv( updated_data, 'outputs/task2_subtaska.csv' ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/task2_subtaska.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLz_mxylfc1e"
      },
      "source": [
        "# Download Submission File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "R6vl3zemfeg6",
        "outputId": "b9519c6b-2fd9-49b6-c582-75dfbb9dfd8b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs/task2_subtaska.csv') \n",
        "## Remeber to put this in a folder called \"submission\"."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0b2add23-bf84-401b-a865-112570e325ba\", \"task2_subtaska.csv\", 32621)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuNKaVgbf2a2"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "cd78IDfLf3Xy",
        "outputId": "585f4392-9f86-4733-f06d-7b56382a0cea"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n{\n            'v': 0.6617513092193399,\n            'f': \"0.6617513092193399\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n{\n            'v': 0.6385562974902141,\n            'f': \"0.6385562974902141\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n{\n            'v': 0.6730190553300397,\n            'f': \"0.6730190553300397\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n{\n            'v': 0.8704885668832538,\n            'f': \"0.8704885668832538\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n{\n            'v': 0.8665745046290478,\n            'f': \"0.8665745046290478\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n{\n            'v': 0.8738206313099324,\n            'f': \"0.8738206313099324\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"number\", \"F1 Score (Macro)\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.661751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.638556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.673019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.870489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.866575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.873821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Settings Languages  F1 Score (Macro)\n",
              "0  zero_shot        EN          0.661751\n",
              "1  zero_shot        PT          0.638556\n",
              "2  zero_shot     EN,PT          0.673019\n",
              "3   one_shot        EN          0.870489\n",
              "4   one_shot        PT          0.866575\n",
              "5   one_shot     EN,PT          0.873821"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEYD3V3I5qEN"
      },
      "source": [
        "Notice the significant jump in F1 scores with the introduction of just one positive and one negative example. \n",
        "\n",
        "Note that your position on the leaderboard will be based on rows with index 2 and 5 (combined results for both languages). The rest of the results for information and ablation studies. \n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}